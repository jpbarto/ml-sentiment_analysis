{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using sentiment analysis to predict stock movement\n",
    "---\n",
    "The following notebook will walk you through using news articles to predict movement of the Dow Jones Industrial Average.  \n",
    "\n",
    "**Notebook outline:**\n",
    "1. Read and examine the Dow Jones end of day data\n",
    "1. Apply logistic regression to evaluate prediction performance\n",
    "1. Read and examine the article data\n",
    "1. Use Amazon Comprehend to tokenize the article data\n",
    "1. Use the tokens to train a SageMaker model to predict stock movement\n",
    "\n",
    "This notebook will rely heavily on the Pandas and SciKitLearn python libraries.  Both libraries have fantastic documentation which you're encouraged to search in support of this notebook's activities.\n",
    "\n",
    "- [Pandas Documentation](https://pandas.pydata.org/pandas-docs/stable/)\n",
    "- [SciKitLearn Documentation](http://scikit-learn.org/stable/modules/)\n",
    "\n",
    "By working through this notebook you will learn to engineer your data in preparation for training machine learning models.  You will also learn to apply ML algorithms from both the SciKitLearn library as well as the Amazon SageMaker service.  You will then independently test the SageMaker endpoint from Python to evaluate model performance is fit for purpose.\n",
    "\n",
    "In addition to the aforementioned libraries you will also leverage the Boto3 Python library to programmatically interact with the AWS API.  For Boto3 documentation please visit https://boto3.readthedocs.io/.\n",
    "\n",
    "### Note: How to get help\n",
    "As you go through this notebook you may need assistance to complete a step or resolve an error message.  In addition to the documentation above you can also use this Notebook to obtain details about different functions.  The Jupyter notebook will respond to the `Tab` press on your keyboard and perform tab-completion of things like variable names, function names, and package names.  In addition if you press Shift-Tab while the cursor is inside the parenthesis of a function the Notebook will display detailed information about the function, its parameters, and return values.  In addition to all these resources please don't forget to ask your friendly neighborhood AWS SA for help if needed.\n",
    "\n",
    "---\n",
    "\n",
    "To get started import the Pandas library and reference as `pd`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Read Dow Jones Industrial Average data\n",
    "\n",
    "Read in, evaluate, and format the data from the DJIA end of day data.  Use the Pandas library to format the data types and manipulate the data so that every row contains the end of day data as well as the next day's High value.\n",
    "\n",
    "Use the 'read_csv' function from the Pandas library to read the data in the CSV file into a Pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "djia_df = pd.read_csv('data/DJIA_table.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the ``shape`` attribute of the Pandas DataFrame object to see how many rows and columns are contained in the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "djia_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the ``describe`` function of the DataFrame to obtain a table describing the mean, standard deviation, and min / max values for each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "djia_df.describe ()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `sample` function of the DataFrame to obtain a number of example values from the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "djia_df.sample (5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `dtypes` attribute of a DataFrame will list the individual data types (as auto-detected) of the columns read in from the CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "djia_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Date` column has been detected as an object or string, reformat it as a datetime data type so that we can more easily work with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "djia_df.Date = pd.to_datetime (djia_df.Date, format='%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `min` and `max` functions of the `Date` column to see the range of dates held within the DJIA EOD data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"{} DJIA EOD data points from {} to {}\".format (djia_df.shape[0], djia_df.Date.min (), djia_df.Date.max ()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DJIA EOD data holds traditional open, close, high, and low data - we are going to predict whether the DJIA has moved up or down on a given day.  To create a label (named `UpDown`) for the data set subtract the opening value from the closing value.  Then test for positive / negative values and use 1 to indicate a rise in DJIA value and a 0 to indicate a falling or unchanged value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "djia_df['UpDown'] = (djia_df.Close - djia_df.Open).astype ('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "djia_df.loc[djia_df.UpDown > 0, 'UpDown'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "djia_df.loc[djia_df.UpDown <= 0, 'UpDown'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly we will be wanting to predict the performance of the DJIA tomorrow, based on today's news.  To do this create another new column named `NextUpDown` which is the previous `UpDown` column but shifted by 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "djia_df = pd.concat ([djia_df, djia_df.UpDown.rename('NextUpDown').shift (-1)], axis=1)\n",
    "# by shifting the UpDown values by 1 we will have a NaN value in one row, remove it\n",
    "djia_df.dropna(axis=0, how='any', inplace=True)\n",
    "djia_df.NextUpDown = djia_df.NextUpDown.astype('int64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `head` function to check the first 5 rows of day in the DataFrame and ensure we have the result desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "djia_df.head (5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To further confirm that the `NextUpDown` column contains a mix of 1s and 0s, and in what proportion, use the `describe` function to summarize the data in the column.  We can see below that it is nearly a 50 / 50 mix of up and down values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "djia_df.describe ()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Train and test simple linear regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's quickly test how well the DJIA EOD data can predict the next day's performance.  Use the SKLearn library to train and evaluate a Logistic Regression model to your data.  Use the first 1500 observations for training and the next 200 observations for testing the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lreg = LogisticRegression ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lreg.fit (X=pd.DataFrame (djia_df.Open[:1500]), y=djia_df.NextUpDown[:1500])\n",
    "lreg.score (X=pd.DataFrame(djia_df.Open[1500:1700]), y=djia_df.NextUpDown[1500:1700])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we are able to accurately predict the following day's movement using only the Opening value from the preceding day.  If we used both the opening and closing value would we gain greater accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lreg.fit (X=pd.concat ([djia_df.Open[:1500], djia_df.Close[:1500]], axis=1), y=djia_df.NextUpDown[:1500])\n",
    "lreg.score (X=pd.concat([djia_df.Open[1500:1700], djia_df.Close[1500:1700]], axis=1), y=djia_df.NextUpDown[1500:1700])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using only the opening and closing values of the previous day we are able to predict with 59% accuracy the performance of the DJIA the next day.  How can we explain this?\n",
    "\n",
    "---\n",
    "# Begin workbook section\n",
    "---\n",
    "## Lets add article data\n",
    "\n",
    "We are able to achieve 59% accuracy in predicting the performance of the DJIA tomorrow, based upon today's open and close values.  Let's see what sort of accuracy we can achieve using today's news to predict tomorrow's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Read article data\n",
    "\n",
    "Read in and evaluate the data contained in the Article CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_df = pd.read_csv ('data/Articles.csv', encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the DataFrame `shape` attribute to determine how many rows and columns are in the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the DataFrame `dtypes` attribute to determine how each column is formatted by data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the DataFrame `sample` function to peek at some example rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Format the `Date` column to be of datetime data type rather than object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `NewsType` column is interesting, lets use the `unique` function of the `NewsType` series to determine what range of possible values this column holds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `min` and `max` functions to determine the range of dates held in the articles data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2692 articles from 2015-01-01 00:00:00 to 2017-03-27 00:00:00\n"
     ]
    }
   ],
   "source": [
    "print (\"{} articles from {} to {}\".format (article_df.shape[0], article_df.Date.min (), article_df.Date.max ()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a collection of over 2500 articles stored in memory.  We could parse these documents manually or using a library but lets use Comprehend to identify the key phrases from each article.\n",
    "\n",
    "---\n",
    "## Parse the articles using Comprehend\n",
    "\n",
    "Using the Amazon Comprehend Boto3 client we will now parse the first 5000 characters from every news article and capture the key phrases identified by Comprehend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "comprehend = boto3.client('comprehend', region_name=region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For every article in the DataFrame pass the article text to Comprehend in order to extract the key phrases for every article.  For every set of key phrases track the phrases retrieved globally as well as at a per-article level.  This will allow you to determine which key phrases are most popular across all articles as well as how many times each key phrase appears in every article."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `iterrows` function of the article DataFrame to pass the article text to Comprehend for processing.  Using this API Comprehend requires that you not exceed 5000 bytes per API call.  For ease lets only pass the first 4900 characters of any one article.\n",
    "\n",
    "For every key phrase returned append the phrase to an array and then `join` the array to a larger array which will hold every article's key phrases.  So for example the larger array should contain something such as:\n",
    "\n",
    "```python\n",
    "[\n",
    "    'key phrase for article 1',\n",
    "    'key phrases for article 2',\n",
    "    'article 3 key phrases'\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_phrase_list = []\n",
    "for index, row in article_df.iterrows ():\n",
    "    key_phrases = comprehend.detect_key_phrases (Text = row['Article'][:4900], LanguageCode='en')\n",
    "    \n",
    "    phrase_strings = []\n",
    "    for phrase in key_phrases['KeyPhrases']:\n",
    "        text = phrase['Text']\n",
    "        phrase_strings.append (text)\n",
    "    key_phrase_list.append (' '.join (phrase_strings))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `Series` constructor convert the large array to a Pandas Series and append it to the article DataFrame using the column name `phrase_string`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When your done your article DataFrame should look similar to the below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Article</th>\n",
       "      <th>Date</th>\n",
       "      <th>Heading</th>\n",
       "      <th>NewsType</th>\n",
       "      <th>phrase_string</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>420</th>\n",
       "      <td>LONDON: The collapse in relations between Saud...</td>\n",
       "      <td>2016-01-06</td>\n",
       "      <td>Saudi Iran split dashes chance of OPEC deal to...</td>\n",
       "      <td>business</td>\n",
       "      <td>LONDON The collapse relations Saudi Arabia and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>839</th>\n",
       "      <td>strong&gt;ISLAMABAD: The Jewellary exports from t...</td>\n",
       "      <td>2016-08-01</td>\n",
       "      <td>Pakistan Jewellary exports up 29 FY16</td>\n",
       "      <td>business</td>\n",
       "      <td>strong&gt;ISLAMABAD The Jewellary exports the cou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439</th>\n",
       "      <td>ISLAMABAD: A five-day Pakistan-China Business ...</td>\n",
       "      <td>2016-01-18</td>\n",
       "      <td>Pak China Business Conference opens in Islamabad</td>\n",
       "      <td>business</td>\n",
       "      <td>ISLAMABAD A five-day Pakistan China Business F...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2521</th>\n",
       "      <td>strong&gt;KARACHI: The State Bank of Pakistan ann...</td>\n",
       "      <td>2016-11-26</td>\n",
       "      <td>State Bank maintains main policy interest rate...</td>\n",
       "      <td>business</td>\n",
       "      <td>strong&gt;KARACHI The State Bank Pakistan its mon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>HONG KONG: Hong Kong stocks edged up 0.24 perc...</td>\n",
       "      <td>2015-01-15</td>\n",
       "      <td>hong kong stocks open 0.24 percent higher</td>\n",
       "      <td>business</td>\n",
       "      <td>HONG KONG Hong Kong stocks 0.24 percent early ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Article       Date  \\\n",
       "420   LONDON: The collapse in relations between Saud... 2016-01-06   \n",
       "839   strong>ISLAMABAD: The Jewellary exports from t... 2016-08-01   \n",
       "439   ISLAMABAD: A five-day Pakistan-China Business ... 2016-01-18   \n",
       "2521  strong>KARACHI: The State Bank of Pakistan ann... 2016-11-26   \n",
       "11    HONG KONG: Hong Kong stocks edged up 0.24 perc... 2015-01-15   \n",
       "\n",
       "                                                Heading  NewsType  \\\n",
       "420   Saudi Iran split dashes chance of OPEC deal to...  business   \n",
       "839               Pakistan Jewellary exports up 29 FY16  business   \n",
       "439    Pak China Business Conference opens in Islamabad  business   \n",
       "2521  State Bank maintains main policy interest rate...  business   \n",
       "11            hong kong stocks open 0.24 percent higher  business   \n",
       "\n",
       "                                          phrase_string  \n",
       "420   LONDON The collapse relations Saudi Arabia and...  \n",
       "839   strong>ISLAMABAD The Jewellary exports the cou...  \n",
       "439   ISLAMABAD A five-day Pakistan China Business F...  \n",
       "2521  strong>KARACHI The State Bank Pakistan its mon...  \n",
       "11    HONG KONG Hong Kong stocks 0.24 percent early ...  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_df.sample (5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Train a LinearLearner on SageMaker using the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may have noticed that you have DJIA EOD data ranging from 2008 to 2016 and article data ranging from 2015 to 2017.  We need to focus on the intersection of these two data sets.  Use the `merge` function of the article DataFrame to join the article DataFrame with the DJIA DataFrame.  Use an `inner` join based upon the `Date` field in both DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When complete you should have a DataFrame with observations ranging from 2015 to 2016-07-01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1448 joined records from 2015-01-02 00:00:00 to 2016-07-01 00:00:00\n"
     ]
    }
   ],
   "source": [
    "print (\"{} joined records from {} to {}\".format (joined_df.shape[0], joined_df.Date.min (), joined_df.Date.max ()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is common in ML development to split a data set into 3 portions.  The training data set will be the largest data set, with approximately 20% of your data held back for validation during training and the last 20% retained for testing purposes post-training.  Create indexes to split your Pandas DataFrame into 60%, 20% and 20% portions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_start = 0\n",
    "validate_start = \n",
    "test_start = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also common practice to shuffle your training set in order to avoid overfitting, especially when performing multiple epochs over the training data.  Use the `shuffle` function of the sklearn.utils module to help shuffle your training set.  Also use the indexes from the last step to create 3 data sets: training_df, validation_df, and test_df.  For each data set also create a reference to the labels for each set: training_labels, validation_labels, and test_labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "train_df = \n",
    "train_labels = \n",
    "validate_df =\n",
    "validate_labels = \n",
    "test_df = \n",
    "test_labels = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the data now split into 3 separate sets use the `CountVectorizer` from the sklearn.features_extraction.text module to create a count of each of the words in each row of `phrase_string`.  Use the `fit_transform` function of the CountVectorizer to convert the `phrase_string` series all at once.  After you have fit the CountVectorizer you can then use the `transform` function to convert the `phrase_string` series of the validation and test data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "wordvec = CountVectorizer ()\n",
    "train_vec = \n",
    "\n",
    "validate_vec =\n",
    "test_vec = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a sanity check create a `LogisticRegression` model from the sklearn.linear_model library to fit to the vectorized training phrases and the training labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lreg = LogisticRegression ()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After fitting the model score it against the validation set.  You should obtain a score of roughly 48%.  Anything less than 50% and you're better off just flipping a coin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally score the model against your test data set and also use the `predict` function of your model to produce a series of predications for every observation in the test data set.  Use the `crosstab` function from Pandas to display a table of false positives and false negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of curiosity peak into the coefficients of your model compared with the words in your vectorizer to see which words have the greatest positive and negative influence over the model's outcome.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Coefficient</th>\n",
       "      <th>Word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>506</th>\n",
       "      <td>0.748372</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>864</th>\n",
       "      <td>0.691797</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>444</th>\n",
       "      <td>0.681982</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6453</th>\n",
       "      <td>0.660790</td>\n",
       "      <td>may</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8961</th>\n",
       "      <td>0.553141</td>\n",
       "      <td>senior</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5471</th>\n",
       "      <td>0.531790</td>\n",
       "      <td>investment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4138</th>\n",
       "      <td>0.522309</td>\n",
       "      <td>fares</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8448</th>\n",
       "      <td>0.511151</td>\n",
       "      <td>review</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>969</th>\n",
       "      <td>0.502739</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2805</th>\n",
       "      <td>0.502160</td>\n",
       "      <td>company</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Coefficient        Word\n",
       "506      0.748372          22\n",
       "864      0.691797          64\n",
       "444      0.681982          20\n",
       "6453     0.660790         may\n",
       "8961     0.553141      senior\n",
       "5471     0.531790  investment\n",
       "4138     0.522309       fares\n",
       "8448     0.511151      review\n",
       "969      0.502739          80\n",
       "2805     0.502160     company"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec_words = wordvec.get_feature_names()\n",
    "lref_coeffs = lreg.coef_.tolist()[0]\n",
    "coeff_df = pd.DataFrame({'Word' : vec_words, \n",
    "                        'Coefficient' : lref_coeffs})\n",
    "coeff_df = coeff_df.sort_values(['Coefficient', 'Word'], ascending=[0, 1])\n",
    "coeff_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Coefficient</th>\n",
       "      <th>Word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2004</th>\n",
       "      <td>-0.511915</td>\n",
       "      <td>biggest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6107</th>\n",
       "      <td>-0.522798</td>\n",
       "      <td>limited</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4416</th>\n",
       "      <td>-0.529109</td>\n",
       "      <td>foreign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5729</th>\n",
       "      <td>-0.531351</td>\n",
       "      <td>karachi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5434</th>\n",
       "      <td>-0.572182</td>\n",
       "      <td>interior</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7824</th>\n",
       "      <td>-0.576874</td>\n",
       "      <td>pressure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7328</th>\n",
       "      <td>-0.603428</td>\n",
       "      <td>oversupply</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5051</th>\n",
       "      <td>-0.647430</td>\n",
       "      <td>hobc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9074</th>\n",
       "      <td>-0.664025</td>\n",
       "      <td>sharjah</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10064</th>\n",
       "      <td>-0.761891</td>\n",
       "      <td>tariff</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Coefficient        Word\n",
       "2004     -0.511915     biggest\n",
       "6107     -0.522798     limited\n",
       "4416     -0.529109     foreign\n",
       "5729     -0.531351     karachi\n",
       "5434     -0.572182    interior\n",
       "7824     -0.576874    pressure\n",
       "7328     -0.603428  oversupply\n",
       "5051     -0.647430        hobc\n",
       "9074     -0.664025     sharjah\n",
       "10064    -0.761891      tariff"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coeff_df.tail (10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Create a model hosted on SageMaker\n",
    "Now in this final step you will train the XGBoost algorithm on SageMaker and host the trained model using a SageMaker endpoint.  To start you will need to write your data sets used earlier out to CSV and upload the CSV documents to S3.\n",
    "\n",
    "The XGBoost algorithm requires that your data be formatted using comma-separated values and that the first value of any observation (the first column) be the observation's label.  So we will want to format our data with the first column being `NextUpDown` and all subsequent values being the word counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = 'sentiment-xgboost'\n",
    "bucket = 'jasbarto-ml-demo'\n",
    "bucket_path = 'https://s3-{}.amazonaws.com/{}'.format(region, bucket)\n",
    "\n",
    "def write_to_s3(fobj, bucket, key):\n",
    "    return boto3.Session().resource('s3').Bucket(bucket).Object(key).upload_fileobj(fobj)\n",
    "\n",
    "def upload_to_s3(bucket, channel, filename):\n",
    "    fobj=open(filename, 'rb')\n",
    "    key = prefix+'/'+channel\n",
    "    url = 's3://{}/{}/{}'.format(bucket, key, filename)\n",
    "    print('Writing to {}'.format(url))\n",
    "    write_to_s3(fobj, bucket, key)\n",
    "\n",
    "train_csv_df = pd.DataFrame (train_vec.todense ())\n",
    "columns = train_csv_df.columns.tolist ()\n",
    "columns.insert (0, 'NextUpDown')\n",
    "train_csv_df['NextUpDown'] = train_df.NextUpDown.values\n",
    "train_csv_df = train_csv_df[columns]\n",
    "\n",
    "\"\"\"\n",
    "Convert the validation data set in a fashion similar to the training set above.\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "Convert the test data set in a fashion similar to the training set above.\n",
    "\"\"\"\n",
    "\n",
    "train_csv_df.to_csv ('train_data.csv', encoding='utf-8', header=False, index=False)\n",
    "\"\"\"\n",
    "Write out the validation and test data sets in a fashion similar to the training set above.\n",
    "\"\"\"\n",
    "\n",
    "upload_to_s3 (bucket, 'train', 'train_data.csv')\n",
    "upload_to_s3 (bucket, 'validation', 'validate_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a SageMaker algorithm\n",
    "The code below will programmatically create a training job for the SageMaker XGBoost algorithm.  Alternatively you can create the training job manually via the AWS web console.\n",
    "\n",
    "**To train via the console:**\n",
    "\n",
    "Submit your data to the XGBoost algorithm via the Amazon SageMaker web console.  Select `Training Jobs` and click the `Create training job` button.\n",
    "\n",
    "Give the training job a name such as 'xgboost-binary-UpDown-classifier' or similar.\n",
    "\n",
    "For `Algorithm` select `XGBoost` from the drop down.\n",
    "\n",
    "Accept the defaults for the remainder of the fields and move down to `Hyperparameters`.  Set the following values for the Hyperparameters accepting the default values for all other parameters.\n",
    "- `num_round` -> 50\n",
    "- `objective` -> 'binary:logistic'\n",
    "\n",
    "For Input data configure two channels.  The first should be given the name 'train' and have the following settings:\n",
    "- `Content-type` -> 'csv'\n",
    "- `Compression type` -> None\n",
    "- `Record wrapper` -> None\n",
    "- `S3 data type` -> S3Prefix\n",
    "- `S3 data distribution type` -> Fully replicated\n",
    "- `S3 location` -> 's3://<your-bucket-name>/<your-model-prefix>/train'\n",
    "\n",
    "For the second channel give it a name of 'validation' and set its parameters the same as the 'train' channel.  Give the 'validation' channel a different `S3 location` however, setting it to 's3://<your-bucket-name><your-model-prefix>/validation'\n",
    "\n",
    "For the Output data configuration set the `S3 output path` to 's3://<your-bucket-name>/<your-model-prefix>/output'.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "import time\n",
    "from time import gmtime, strftime\n",
    "\n",
    "role = get_execution_role()\n",
    "\n",
    "container = get_image_uri(boto3.Session().region_name, 'xgboost')\n",
    "\n",
    "job_name = 'DEMO-xgboost-regression-' + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "print(\"Training job\", job_name)\n",
    "\n",
    "#Ensure that the training and validation data folders generated above are reflected in the \"InputDataConfig\" parameter below.\n",
    "\n",
    "create_training_params = \\\n",
    "{\n",
    "    \"AlgorithmSpecification\": {\n",
    "        \"TrainingImage\": container,\n",
    "        \"TrainingInputMode\": \"File\"\n",
    "    },\n",
    "    \"RoleArn\": role,\n",
    "    \"OutputDataConfig\": {\n",
    "        \"S3OutputPath\": bucket_path + \"/\" + prefix + \"/single-boost\"\n",
    "    },\n",
    "    \"ResourceConfig\": {\n",
    "        \"InstanceCount\": 1,\n",
    "        \"InstanceType\": \"ml.m4.4xlarge\",\n",
    "        \"VolumeSizeInGB\": 5\n",
    "    },\n",
    "    \"TrainingJobName\": job_name,\n",
    "    \"HyperParameters\": {\n",
    "        \"max_depth\":\"5\",\n",
    "        \"eta\":\"0.2\",\n",
    "        \"gamma\":\"4\",\n",
    "        \"min_child_weight\":\"6\",\n",
    "        \"subsample\":\"0.7\",\n",
    "        \"silent\":\"0\",\n",
    "        \"objective\":\"binary:logistic\",\n",
    "        \"num_round\":\"50\"\n",
    "    },\n",
    "    \"StoppingCondition\": {\n",
    "        \"MaxRuntimeInSeconds\": 3600\n",
    "    },\n",
    "    \"InputDataConfig\": [\n",
    "        {\n",
    "            \"ChannelName\": \"train\",\n",
    "            \"DataSource\": {\n",
    "                \"S3DataSource\": {\n",
    "                    \"S3DataType\": \"S3Prefix\",\n",
    "                    \"S3Uri\": bucket_path + \"/\" + prefix + '/train',\n",
    "                    \"S3DataDistributionType\": \"FullyReplicated\"\n",
    "                }\n",
    "            },\n",
    "            \"ContentType\": \"csv\",\n",
    "            \"CompressionType\": \"None\"\n",
    "        },\n",
    "        {\n",
    "            \"ChannelName\": \"validation\",\n",
    "            \"DataSource\": {\n",
    "                \"S3DataSource\": {\n",
    "                    \"S3DataType\": \"S3Prefix\",\n",
    "                    \"S3Uri\": bucket_path + \"/\" + prefix + '/validation',\n",
    "                    \"S3DataDistributionType\": \"FullyReplicated\"\n",
    "                }\n",
    "            },\n",
    "            \"ContentType\": \"csv\",\n",
    "            \"CompressionType\": \"None\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "\n",
    "client = boto3.client('sagemaker')\n",
    "client.create_training_job(**create_training_params)\n",
    "\n",
    "status = client.describe_training_job(TrainingJobName=job_name)['TrainingJobStatus']\n",
    "print(status)\n",
    "while status !='Completed' and status!='Failed':\n",
    "    time.sleep(60)\n",
    "    status = client.describe_training_job(TrainingJobName=job_name)['TrainingJobStatus']\n",
    "    print(status)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a model from training output\n",
    "\n",
    "The preceding training job, when complete, will produce a model export into the configured `S3 output path`.  We will now use that output to create a model which can then be hosted by SageMaker.  The creation of a model can be done programmatically using the code below, alternatively you can create a model manually via the SageMaker web console.\n",
    "\n",
    "From the SageMaker web console `Inference` -> `Models`.  Then click `Create model`.\n",
    "\n",
    "Give the model a name such as **'xgboost-model'** and set the `Location of model artifacts` to your output directory.  For example: **`s3://<your-bucket-name>/<your-model-prefix>/output/model.tar.gz`**.  This same value will also be listed in the summary output of the preceding training job.  For the `Location of inference code image` set the value of **'685385470294.dkr.ecr.eu-west-1.amazonaws.com/xgboost:1'**.\n",
    "\n",
    "Click `Create model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "model_name=job_name + '-model'\n",
    "print(model_name)\n",
    "\n",
    "info = client.describe_training_job(TrainingJobName=job_name)\n",
    "model_data = info['ModelArtifacts']['S3ModelArtifacts']\n",
    "print(model_data)\n",
    "\n",
    "primary_container = {\n",
    "    'Image': container,\n",
    "    'ModelDataUrl': model_data\n",
    "}\n",
    "\n",
    "create_model_response = client.create_model(\n",
    "    ModelName = model_name,\n",
    "    ExecutionRoleArn = role,\n",
    "    PrimaryContainer = primary_container)\n",
    "\n",
    "print(create_model_response['ModelArn'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create endpoint configuration\n",
    "\n",
    "SageMaker hosts your model as an endpoint.  This endpoint must be configured with which models you would like it to host.  To create an endpoint configuration you can use the code below to create one programmatically or create one manually via the web console.\n",
    "\n",
    "From the SageMaker web console click `Endpoint configurations` under `Inference`.  Click `Create endpoint configuration`.\n",
    "\n",
    "Give your endpoint configuration a name such as 'xgboost-model-endpoint-config' and under 'Production variants' click `Add model`.  Select the model you created in the previous step and click `Create endpoint configuration`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_config_name = 'DEMO-XGBoostEndpointConfig-' + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "print(endpoint_config_name)\n",
    "create_endpoint_config_response = client.create_endpoint_config(\n",
    "    EndpointConfigName = endpoint_config_name,\n",
    "    ProductionVariants=[{\n",
    "        'InstanceType':'ml.m4.xlarge',\n",
    "        'InitialVariantWeight':1,\n",
    "        'InitialInstanceCount':1,\n",
    "        'ModelName':model_name,\n",
    "        'VariantName':'AllTraffic'}])\n",
    "\n",
    "print(\"Endpoint Config Arn: \" + create_endpoint_config_response['EndpointConfigArn'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy endpoint\n",
    "\n",
    "Finally deploy the configured endpoint so that it can be invoked as a secure, RESTful endpoint.  To deploy the endpoint you can use the code below or create a deployment manually via the web console.\n",
    "\n",
    "From the SageMaker web console click `Endpoints` under `Inference`.  Click `Create endpoint`.  Give your endpoint a name such as `xgboost-endpoint` and select the endpoint configuration created in the previous step.  Click `Deploy endpoint`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "endpoint_name = 'DEMO-XGBoostEndpoint-' + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "print(endpoint_name)\n",
    "create_endpoint_response = client.create_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    EndpointConfigName=endpoint_config_name)\n",
    "print(create_endpoint_response['EndpointArn'])\n",
    "\n",
    "resp = client.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = resp['EndpointStatus']\n",
    "print(\"Status: \" + status)\n",
    "\n",
    "while status=='Creating':\n",
    "    time.sleep(60)\n",
    "    resp = client.describe_endpoint(EndpointName=endpoint_name)\n",
    "    status = resp['EndpointStatus']\n",
    "    print(\"Status: \" + status)\n",
    "\n",
    "print(\"Arn: \" + resp['EndpointArn'])\n",
    "print(\"Status: \" + status)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Invoke the endpoint\n",
    "\n",
    "Once the SageMaker endpoint has been deployed your trained model is running and ready to perform inference.  Let's test it now using the Boto3 client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_client = boto3.client('runtime.sagemaker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -n1 test_data.csv > /tmp/test_data_single.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import json\n",
    "from itertools import islice\n",
    "import math\n",
    "import struct\n",
    "    \n",
    "def predict (features):\n",
    "    response = sm_client.invoke_endpoint(EndpointName=endpoint_name, \n",
    "                                   ContentType='text/csv', \n",
    "                                   Body=features)\n",
    "    result = response['Body'].read()\n",
    "    result = 1 if float(result) > 0.5 else 0\n",
    "    return result\n",
    "\n",
    "file_name = '/tmp/test_data_single.csv' #customize to your test file\n",
    "\n",
    "with open(file_name, 'r') as f:\n",
    "    payload = f.read().strip()\n",
    "\n",
    "print (\"Label: {}\\nPrediction: {}\".format (payload[0:1], predict (payload[2:])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the convenience function defined above iterate over the contents of the `test_data.csv` file and record the prediction for each observation.  The first number of every observation is the label for that observation, store the label, along with the prediction to then compare them as a measure of the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = []\n",
    "test_predictions = []\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the `accuracy_score` function from the `sklearn.metrics` module.  Use the `accuracy_score` function to calculate the accuracy of the model against the labels and predictions collected in the previous step.\n",
    "\n",
    "Next use the `crosstab` function of the Pandas library to produce a formatted table highlighting the number of false positives and false negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Complete\n",
    "\n",
    "That completes this lab.  You should have trained and created a hosted SageMaker model using the XGBoost algorithm for classifying stock movements using sentiment analysis.  The accuracy of your model should have been approximately 52%.  Now think about how you could improve the accuracy of the model.  What are some of the things you could do to perhaps the data, the algorithm, or the algorithm parameters, to tune the performance of your trained model?\n",
    "\n",
    "### Note: Remove created resources\n",
    "As part of this lab you will have created an S3 bucket, a SageMaker notebook server, and at least one training job, model, and endpoint.  Please be sure to create each of these to avoid incurring any further charges to your AWS account."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
